{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73c5cc6",
   "metadata": {},
   "source": [
    "# ChatOllama Tool Calling Example: Simple Calculator\n",
    "\n",
    "This notebook demonstrates how to define a simple calculator as a tool and let ChatOllama call it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b053ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Python functions\n",
    "def add(a, b): return a + b\n",
    "def subtract(a, b): return a - b\n",
    "def multiply(a, b): \n",
    "    print(\"--------------->\")\n",
    "    return a * b\n",
    "def divide(a, b): return a / b if b != 0 else \"Error: division by zero\"\n",
    "\n",
    "# Tools\n",
    "tools = [\n",
    "    Tool(name=\"Add\", func=add, description=\"Adds two numbers a and b\"),\n",
    "    Tool(name=\"Subtract\", func=subtract, description=\"Subtracts b from a\"),\n",
    "    Tool(name=\"Multiply\", func=multiply, description=\"Multiplies a and b\"),\n",
    "    Tool(name=\"Divide\", func=divide, description=\"Divides a by b\")\n",
    "]\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    \n",
    "    )\n",
    "\n",
    "# Agent\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb9a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test a simple tool call\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m agent.astream(\u001b[33m\"\u001b[39m\u001b[33mCalculate 45 + 67 using Multiply tool\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      4\u001b[39m             \u001b[38;5;66;03m# append token if present\u001b[39;00m\n\u001b[32m      5\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m chunk.content:\n\u001b[32m      6\u001b[39m                 response_text += chunk.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1827\u001b[39m, in \u001b[36mAgentExecutor.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1815\u001b[39m config = ensure_config(config)\n\u001b[32m   1816\u001b[39m iterator = AgentExecutorIterator(\n\u001b[32m   1817\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1825\u001b[39m     **kwargs,\n\u001b[32m   1826\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1827\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m   1828\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m step\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent_iterator.py:268\u001b[39m, in \u001b[36mAgentExecutorIterator.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_executor._should_continue(\n\u001b[32m    262\u001b[39m     \u001b[38;5;28mself\u001b[39m.iterations,\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mself\u001b[39m.time_elapsed,\n\u001b[32m    264\u001b[39m ):\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# take the next step: this plans next action, executes it,\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# yielding action and observation as they are generated\u001b[39;00m\n\u001b[32m    267\u001b[39m     next_step_seq: NextStepOutput = []\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_executor._aiter_next_step(\n\u001b[32m    269\u001b[39m         \u001b[38;5;28mself\u001b[39m.name_to_tool_map,\n\u001b[32m    270\u001b[39m         \u001b[38;5;28mself\u001b[39m.color_mapping,\n\u001b[32m    271\u001b[39m         \u001b[38;5;28mself\u001b[39m.inputs,\n\u001b[32m    272\u001b[39m         \u001b[38;5;28mself\u001b[39m.intermediate_steps,\n\u001b[32m    273\u001b[39m         run_manager,\n\u001b[32m    274\u001b[39m     ):\n\u001b[32m    275\u001b[39m         next_step_seq.append(chunk)\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# if we're yielding actions, yield them as they come\u001b[39;00m\n\u001b[32m    277\u001b[39m         \u001b[38;5;66;03m# do not yield AgentFinish, which will be handled below\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1491\u001b[39m, in \u001b[36mAgentExecutor._aiter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1488\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1490\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._action_agent.aplan(\n\u001b[32m   1492\u001b[39m         intermediate_steps,\n\u001b[32m   1493\u001b[39m         callbacks=run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1494\u001b[39m         **inputs,\n\u001b[32m   1495\u001b[39m     )\n\u001b[32m   1496\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:821\u001b[39m, in \u001b[36mAgent.aplan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Async given input, decided what to do.\u001b[39;00m\n\u001b[32m    810\u001b[39m \n\u001b[32m    811\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    818\u001b[39m \u001b[33;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[32m    819\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    820\u001b[39m full_inputs = \u001b[38;5;28mself\u001b[39m.get_full_inputs(intermediate_steps, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m full_output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_chain.apredict(callbacks=callbacks, **full_inputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_parser.aparse(full_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:342\u001b[39m, in \u001b[36mLLMChain.apredict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    328\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    329\u001b[39m \n\u001b[32m    330\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.acall(kwargs, callbacks=callbacks))[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:201\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.awarning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    200\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m wrapped(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:460\u001b[39m, in \u001b[36mChain.acall\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Asynchronously execute the chain.\u001b[39;00m\n\u001b[32m    430\u001b[39m \n\u001b[32m    431\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    452\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    454\u001b[39m config = {\n\u001b[32m    455\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    456\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    457\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    458\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    459\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ainvoke(\n\u001b[32m    461\u001b[39m     inputs,\n\u001b[32m    462\u001b[39m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[32m    463\u001b[39m     return_only_outputs=return_only_outputs,\n\u001b[32m    464\u001b[39m     include_run_info=include_run_info,\n\u001b[32m    465\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:220\u001b[39m, in \u001b[36mChain.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    218\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    219\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acall(inputs, run_manager=run_manager)\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acall(inputs)\n\u001b[32m    223\u001b[39m     )\n\u001b[32m    224\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aprep_outputs(\n\u001b[32m    225\u001b[39m         inputs,\n\u001b[32m    226\u001b[39m         outputs,\n\u001b[32m    227\u001b[39m         return_only_outputs,\n\u001b[32m    228\u001b[39m     )\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:307\u001b[39m, in \u001b[36mLLMChain._acall\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_acall\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    304\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    305\u001b[39m     run_manager: Optional[AsyncCallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    306\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate([inputs], run_manager=run_manager)\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:166\u001b[39m, in \u001b[36mLLMChain.agenerate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    164\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.agenerate_prompt(\n\u001b[32m    167\u001b[39m         prompts,\n\u001b[32m    168\u001b[39m         stop,\n\u001b[32m    169\u001b[39m         callbacks=callbacks,\n\u001b[32m    170\u001b[39m         **\u001b[38;5;28mself\u001b[39m.llm_kwargs,\n\u001b[32m    171\u001b[39m     )\n\u001b[32m    172\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).abatch(\n\u001b[32m    173\u001b[39m     cast(\u001b[38;5;28mlist\u001b[39m, prompts),\n\u001b[32m    174\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks},\n\u001b[32m    175\u001b[39m )\n\u001b[32m    176\u001b[39m generations: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Generation]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1034\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     **kwargs: Any,\n\u001b[32m   1032\u001b[39m ) -> LLMResult:\n\u001b[32m   1033\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1035\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1036\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:954\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m    942\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    943\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     run_id=run_id,\n\u001b[32m    949\u001b[39m )\n\u001b[32m    951\u001b[39m input_messages = [\n\u001b[32m    952\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    953\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    955\u001b[39m     *[\n\u001b[32m    956\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m    957\u001b[39m             m,\n\u001b[32m    958\u001b[39m             stop=stop,\n\u001b[32m    959\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    960\u001b[39m             **kwargs,\n\u001b[32m    961\u001b[39m         )\n\u001b[32m    962\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m    963\u001b[39m     ],\n\u001b[32m    964\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    965\u001b[39m )\n\u001b[32m    966\u001b[39m exceptions = []\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1162\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1160\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1161\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1163\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1164\u001b[39m     )\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:998\u001b[39m, in \u001b[36mChatOllama._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_agenerate\u001b[39m(\n\u001b[32m    992\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    993\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    996\u001b[39m     **kwargs: Any,\n\u001b[32m    997\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m     final_chunk = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._achat_stream_with_aggregation(\n\u001b[32m    999\u001b[39m         messages, stop, run_manager, verbose=\u001b[38;5;28mself\u001b[39m.verbose, **kwargs\n\u001b[32m   1000\u001b[39m     )\n\u001b[32m   1001\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1002\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1003\u001b[39m         message=AIMessage(\n\u001b[32m   1004\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1009\u001b[39m         generation_info=generation_info,\n\u001b[32m   1010\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:785\u001b[39m, in \u001b[36mChatOllama._achat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_achat_stream_with_aggregation\u001b[39m(\n\u001b[32m    777\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    778\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    782\u001b[39m     **kwargs: Any,\n\u001b[32m    783\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    784\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiterate_over_stream(messages, stop, **kwargs):\n\u001b[32m    786\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    787\u001b[39m             final_chunk = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:922\u001b[39m, in \u001b[36mChatOllama._aiterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_aiterate_over_stream\u001b[39m(\n\u001b[32m    916\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    917\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    918\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    919\u001b[39m     **kwargs: Any,\n\u001b[32m    920\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m    921\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acreate_chat_stream(messages, stop, **kwargs):\n\u001b[32m    923\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    924\u001b[39m             content = (\n\u001b[32m    925\u001b[39m                 stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    926\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    927\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    928\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:730\u001b[39m, in \u001b[36mChatOllama._acreate_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat(**chat_params):\n\u001b[32m    731\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\ollama\\_client.py:684\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m    682\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n\u001b[32m    685\u001b[39m   part = json.loads(line)\n\u001b[32m    686\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m err := part.get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_models.py:1031\u001b[39m, in \u001b[36mResponse.aiter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1029\u001b[39m decoder = LineDecoder()\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_text():\n\u001b[32m   1032\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder.decode(text):\n\u001b[32m   1033\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_models.py:1018\u001b[39m, in \u001b[36mResponse.aiter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1016\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_bytes():\n\u001b[32m   1019\u001b[39m         text_content = decoder.decode(byte_content)\n\u001b[32m   1020\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(text_content):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_models.py:997\u001b[39m, in \u001b[36mResponse.aiter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    995\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_raw():\n\u001b[32m    998\u001b[39m         decoded = decoder.decode(raw_bytes)\n\u001b[32m    999\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(decoded):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_models.py:1055\u001b[39m, in \u001b[36mResponse.aiter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1052\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_bytes_downloaded += \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(raw_stream_bytes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_client.py:176\u001b[39m, in \u001b[36mBoundAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:271\u001b[39m, in \u001b[36mAsyncResponseStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._httpcore_stream:\n\u001b[32m    272\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    404\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection._receive_response_body(**kwargs):\n\u001b[32m    335\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:203\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:1254\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1249\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1252\u001b[39m ):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\asyncio\\locks.py:213\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize ChatOllama\n",
    "chat = ChatOllama(model=\"deepseek-r1:8b\")  # replace with your model\n",
    "\n",
    "# Test a simple tool call\n",
    "response = chat.invoke(\"Calculate 45 multiplied by 67 using the Multiply tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92917322",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'tool_calls'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Inspect the tool calls\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_calls\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'tool_calls'"
     ]
    }
   ],
   "source": [
    "# Inspect the tool calls\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e55593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract the tool arguments and calculate the result manually\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    args = tool_call['args']\n",
    "    result = args['a'] * args['b']\n",
    "    print(f\"Tool called: {tool_call['name']}\")\n",
    "    print(f\"Arguments: a={args['a']}, b={args['b']}\")\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `<think>\nTo calculate \\(45 \\times 67\\) using multiplication, I'll break it down into smaller parts for easier computation.\n\nFirst, multiply 45 by 60:\n\\(45 \\times 60 = 2700\\).\n\nNext, multiply 45 by 7:\n\\(45 \\times 7 = 315\\).\n\nFinally, add the two results together to get the total product:\n\\(2700 + 315 = 3015\\).\n</think>\n\nTo calculate \\(45 \\times 67\\) using the Multiply tool:\n\n**Step 1:**  \nMultiply 45 by 60.\n\\[ 45 \\times 60 = 2700 \\]\n\n**Step 2:**  \nMultiply 45 by 7.\n\\[ 45 \\times 7 = 315 \\]\n\n**Step 3:**  \nAdd the results from Step 1 and Step 2.\n\\[ 2700 + 315 = 3015 \\]\n\n\\[\n\\boxed{3015}\n\\]`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1491\u001b[39m, in \u001b[36mAgentExecutor._aiter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1490\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._action_agent.aplan(\n\u001b[32m   1492\u001b[39m         intermediate_steps,\n\u001b[32m   1493\u001b[39m         callbacks=run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1494\u001b[39m         **inputs,\n\u001b[32m   1495\u001b[39m     )\n\u001b[32m   1496\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:822\u001b[39m, in \u001b[36mAgent.aplan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m full_output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_chain.apredict(callbacks=callbacks, **full_inputs)\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_parser.aparse(full_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:295\u001b[39m, in \u001b[36mBaseOutputParser.aparse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Async parse a single string model output into some structure.\u001b[39;00m\n\u001b[32m    288\u001b[39m \n\u001b[32m    289\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    293\u001b[39m \u001b[33;03m    Structured output.\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m.parse, text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:611\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(executor_or_config, func, *args, **kwargs)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    610\u001b[39m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(\n\u001b[32m    612\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    613\u001b[39m         cast(\u001b[33m\"\u001b[39m\u001b[33mCallable[..., T]\u001b[39m\u001b[33m\"\u001b[39m, partial(copy_context().run, wrapper)),\n\u001b[32m    614\u001b[39m     )\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:602\u001b[39m, in \u001b[36mrun_in_executor.<locals>.wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    604\u001b[39m     \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[32m    605\u001b[39m     \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[32m    606\u001b[39m     \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\mrkl\\output_parser.py:80\u001b[39m, in \u001b[36mMRKLOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     79\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[32m     81\u001b[39m         msg,\n\u001b[32m     82\u001b[39m         observation=MISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[32m     83\u001b[39m         llm_output=text,\n\u001b[32m     84\u001b[39m         send_to_llm=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     85\u001b[39m     )\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re.search(\n\u001b[32m     87\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]*Action\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*Input\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*:[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]*(.*)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m     text,\n\u001b[32m     89\u001b[39m     re.DOTALL,\n\u001b[32m     90\u001b[39m ):\n",
      "\u001b[31mOutputParserException\u001b[39m: Could not parse LLM output: `<think>\nTo calculate \\(45 \\times 67\\) using multiplication, I'll break it down into smaller parts for easier computation.\n\nFirst, multiply 45 by 60:\n\\(45 \\times 60 = 2700\\).\n\nNext, multiply 45 by 7:\n\\(45 \\times 7 = 315\\).\n\nFinally, add the two results together to get the total product:\n\\(2700 + 315 = 3015\\).\n</think>\n\nTo calculate \\(45 \\times 67\\) using the Multiply tool:\n\n**Step 1:**  \nMultiply 45 by 60.\n\\[ 45 \\times 60 = 2700 \\]\n\n**Step 2:**  \nMultiply 45 by 7.\n\\[ 45 \\times 7 = 315 \\]\n\n**Step 3:**  \nAdd the results from Step 1 and Step 2.\n\\[ 2700 + 315 = 3015 \\]\n\n\\[\n\\boxed{3015}\n\\]`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Stream\u001b[39;00m\n\u001b[32m     26\u001b[39m response_text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m agent.astream(\u001b[33m\"\u001b[39m\u001b[33mCalculate 45 * 67 using Multiply tool\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.content:\n\u001b[32m     29\u001b[39m         response_text += chunk.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1827\u001b[39m, in \u001b[36mAgentExecutor.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1815\u001b[39m config = ensure_config(config)\n\u001b[32m   1816\u001b[39m iterator = AgentExecutorIterator(\n\u001b[32m   1817\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1825\u001b[39m     **kwargs,\n\u001b[32m   1826\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1827\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m   1828\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m step\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent_iterator.py:268\u001b[39m, in \u001b[36mAgentExecutorIterator.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_executor._should_continue(\n\u001b[32m    262\u001b[39m     \u001b[38;5;28mself\u001b[39m.iterations,\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mself\u001b[39m.time_elapsed,\n\u001b[32m    264\u001b[39m ):\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# take the next step: this plans next action, executes it,\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# yielding action and observation as they are generated\u001b[39;00m\n\u001b[32m    267\u001b[39m     next_step_seq: NextStepOutput = []\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_executor._aiter_next_step(\n\u001b[32m    269\u001b[39m         \u001b[38;5;28mself\u001b[39m.name_to_tool_map,\n\u001b[32m    270\u001b[39m         \u001b[38;5;28mself\u001b[39m.color_mapping,\n\u001b[32m    271\u001b[39m         \u001b[38;5;28mself\u001b[39m.inputs,\n\u001b[32m    272\u001b[39m         \u001b[38;5;28mself\u001b[39m.intermediate_steps,\n\u001b[32m    273\u001b[39m         run_manager,\n\u001b[32m    274\u001b[39m     ):\n\u001b[32m    275\u001b[39m         next_step_seq.append(chunk)\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# if we're yielding actions, yield them as they come\u001b[39;00m\n\u001b[32m    277\u001b[39m         \u001b[38;5;66;03m# do not yield AgentFinish, which will be handled below\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1508\u001b[39m, in \u001b[36mAgentExecutor._aiter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[32m   1502\u001b[39m     msg = (\n\u001b[32m   1503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn output parsing error occurred. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1505\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1506\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1507\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1508\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1509\u001b[39m text = \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `<think>\nTo calculate \\(45 \\times 67\\) using multiplication, I'll break it down into smaller parts for easier computation.\n\nFirst, multiply 45 by 60:\n\\(45 \\times 60 = 2700\\).\n\nNext, multiply 45 by 7:\n\\(45 \\times 7 = 315\\).\n\nFinally, add the two results together to get the total product:\n\\(2700 + 315 = 3015\\).\n</think>\n\nTo calculate \\(45 \\times 67\\) using the Multiply tool:\n\n**Step 1:**  \nMultiply 45 by 60.\n\\[ 45 \\times 60 = 2700 \\]\n\n**Step 2:**  \nMultiply 45 by 7.\n\\[ 45 \\times 7 = 315 \\]\n\n**Step 3:**  \nAdd the results from Step 1 and Step 2.\n\\[ 2700 + 315 = 3015 \\]\n\n\\[\n\\boxed{3015}\n\\]`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Tools\n",
    "async def multiply_async(a, b):\n",
    "    import asyncio\n",
    "    await asyncio.sleep(0.1)  # simulate async work\n",
    "    return a * b\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Multiply\", func=multiply_async, description=\"Multiplies two numbers\")\n",
    "]\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"deepseek-r1:7b\")\n",
    "\n",
    "# Agent\n",
    "agent = initialize_agent(\n",
    "    \n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Stream\n",
    "response_text = \"\"\n",
    "async for chunk in agent.astream(\"Calculate 45 * 67 using Multiply tool\"):\n",
    "    if chunk.content:\n",
    "        response_text += chunk.content\n",
    "        print(chunk.content, end=\"\")  # streaming output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e84061",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "- We define a tool as a **Pydantic model** (`Multiply`) with fields for the inputs.\n",
    "- ChatOllama can **call tools automatically** if the prompt indicates the tool usage.\n",
    "- The `tool_calls` field in the response contains the tool name and arguments.\n",
    "- You can then execute the tool logic in Python or elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d246a",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "- Add more operations like Add, Subtract, Divide as separate tools.\n",
    "- Combine with **streaming** to see partial tool call results.\n",
    "- Use JSON output format if you want structured responses for frontend consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a32197c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ReACTAgent' from 'langchain.agents.react.base' (d:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\react\\base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m llm = ChatOllama(model=\u001b[33m\"\u001b[39m\u001b[33mdeepseek-r1:7b\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Create an agent manually\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreact\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReACTAgent\n\u001b[32m     20\u001b[39m agent = ReACTAgent.from_llm_and_tools(llm=llm, tools=tools)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Wrap in AgentExecutor\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ReACTAgent' from 'langchain.agents.react.base' (d:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\react\\base.py)"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.agents import Tool\n",
    "\n",
    "# Example tools\n",
    "def add(a, b): return a + b\n",
    "def multiply(a, b): return a * b\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Add\", func=add, description=\"Add two numbers\"),\n",
    "    Tool(name=\"Multiply\", func=multiply, description=\"Multiply two numbers\")\n",
    "]\n",
    "\n",
    "# Your LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"deepseek-r1:7b\")\n",
    "\n",
    "# Create an agent manually\n",
    "from langchain.agents.react.base import ReACTAgent\n",
    "agent = ReACTAgent.from_llm_and_tools(llm=llm, tools=tools)\n",
    "\n",
    "# Wrap in AgentExecutor\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run\n",
    "result = executor.run(\"Calculate 45 * 67 using Multiply tool\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c25536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Streaming tool call ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python313\\Lib\\dis.py:205: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  def _deoptop(op):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "registry.ollama.ai/library/deepseek-r1:7b does not support tools (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m             \u001b[38;5;28mprint\u001b[39m(chunk.tool_calls)\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# You would continue here with the logic to execute the tool\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# and stream the result back to the model.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m messages = [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the weather like in New York?\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Streaming tool call ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m llm_with_tools.astream(messages):\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# The output chunk will contain the tool_calls attribute\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.tool_calls:\n\u001b[32m     34\u001b[39m         \u001b[38;5;28mprint\u001b[39m(chunk.tool_calls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5900\u001b[39m, in \u001b[36mRunnableBindingBase.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5893\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5894\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mastream\u001b[39m(\n\u001b[32m   5895\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5898\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5899\u001b[39m ) -> AsyncIterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m5900\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.astream(\n\u001b[32m   5901\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5902\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5903\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5904\u001b[39m     ):\n\u001b[32m   5905\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:615\u001b[39m, in \u001b[36mBaseChatModel.astream\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    613\u001b[39m input_messages = _normalize_messages(messages)\n\u001b[32m    614\u001b[39m run_id = \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m.join((_LC_ID_PREFIX, \u001b[38;5;28mstr\u001b[39m(run_manager.run_id)))\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._astream(\n\u001b[32m    616\u001b[39m     input_messages,\n\u001b[32m    617\u001b[39m     stop=stop,\n\u001b[32m    618\u001b[39m     **kwargs,\n\u001b[32m    619\u001b[39m ):\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.message.id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    621\u001b[39m         chunk.message.id = run_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:983\u001b[39m, in \u001b[36mChatOllama._astream\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_astream\u001b[39m(\n\u001b[32m    977\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    978\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    981\u001b[39m     **kwargs: Any,\n\u001b[32m    982\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiterate_over_stream(messages, stop, **kwargs):\n\u001b[32m    984\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n\u001b[32m    985\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_llm_new_token(\n\u001b[32m    986\u001b[39m                 chunk.text,\n\u001b[32m    987\u001b[39m                 verbose=\u001b[38;5;28mself\u001b[39m.verbose,\n\u001b[32m    988\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:922\u001b[39m, in \u001b[36mChatOllama._aiterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_aiterate_over_stream\u001b[39m(\n\u001b[32m    916\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    917\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    918\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    919\u001b[39m     **kwargs: Any,\n\u001b[32m    920\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m    921\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acreate_chat_stream(messages, stop, **kwargs):\n\u001b[32m    923\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    924\u001b[39m             content = (\n\u001b[32m    925\u001b[39m                 stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    926\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    927\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    928\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:730\u001b[39m, in \u001b[36mChatOllama._acreate_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat(**chat_params):\n\u001b[32m    731\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\ollama\\_client.py:682\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n\u001b[32m    685\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: registry.ollama.ai/library/deepseek-r1:7b does not support tools (status code: 400)"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# IMPORTANT: Use the new import path\n",
    "from langchain_ollama import ChatOllama \n",
    "\n",
    "# 1. Define your tools\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Gets the current weather for a specified city.\"\"\"\n",
    "    if city.lower() == \"new york\":\n",
    "        return \"It's 24°C and sunny in New York. ☀️\"\n",
    "    elif city.lower() == \"london\":\n",
    "        return \"It's 15°C and cloudy in London. ☁️\"\n",
    "    else:\n",
    "        return f\"I don't have the weather for {city}.\"\n",
    "\n",
    "# 2. Initialize the model and bind the tools\n",
    "# Make sure your Ollama server is running with a model that supports tool calling\n",
    "# The model name here is an example; use the one you have, e.g., \"llama3\"\n",
    "llm = ChatOllama(model=\"deepseek-r1:7b\", temperature=0)\n",
    "tools = [get_weather]\n",
    "llm_with_tools = llm.bind_tools(tools) # This will now work correctly\n",
    "\n",
    "# The rest of your async main function remains the same...\n",
    "async def main():\n",
    "    messages = [HumanMessage(content=\"What's the weather like in New York?\")]\n",
    "    \n",
    "    print(\"--- Streaming tool call ---\")\n",
    "    async for chunk in llm_with_tools.astream(messages):\n",
    "        # The output chunk will contain the tool_calls attribute\n",
    "        if chunk.tool_calls:\n",
    "            print(chunk.tool_calls)\n",
    "    \n",
    "    # You would continue here with the logic to execute the tool\n",
    "    # and stream the result back to the model.\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62dcdc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOOL START: multiply with args {'a': '8', 'b': '10'} ---\n",
      "--- TOOL: Executing multiply(8, 10) ---\n",
      "\n",
      "--- TOOL END: Tool multiply output was: 80 ---\n",
      "The result of 3 + 5 is 8. Multiplying this result by 10 gives us a final answer of 80."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "# Imports are slightly changed\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool # <-- Import the decorator\n",
    "\n",
    "# Add the @tool decorator above each function\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers a and b.\"\"\"\n",
    "    print(f\"--- TOOL: Executing multiply({a}, {b}) ---\")\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers a and b.\"\"\"\n",
    "    print(f\"--- TOOL: Executing add({a}, {b}) ---\")\n",
    "    return a + b\n",
    "\n",
    "# model=\"phi3:mini\"\n",
    "model = \"llama3.1:8b\"\n",
    "    \n",
    "# 1. Choose a tool-calling model\n",
    "llm = ChatOllama(model= model, temperature=0)\n",
    "\n",
    "# The tools list is now simpler\n",
    "tools = [add, multiply]\n",
    "\n",
    "# 2. Create the Agent using a modern prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that is good at math.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# 3. Create the Agent Executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "\n",
    "\n",
    "async def stream_agent_response():\n",
    "    \"\"\"\n",
    "    Streams the agent's response, including intermediate steps.\n",
    "    \"\"\"\n",
    "    async for event in agent_executor.astream_events(\n",
    "        {\"input\": \"What is 3 plus 5, and then multiply the result by 10?\"},\n",
    "        version=\"v2\"\n",
    "    ):\n",
    "        event_type = event[\"event\"]\n",
    "        \n",
    "        if event_type == \"on_chat_model_stream\":\n",
    "            chunk = event[\"data\"][\"chunk\"]\n",
    "            if chunk.content:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "        elif event_type == \"on_tool_start\":\n",
    "            print(f\"\\n--- TOOL START: {event['name']} with args {event['data'].get('input')} ---\")\n",
    "            \n",
    "        elif event_type == \"on_tool_end\":\n",
    "            print(f\"\\n--- TOOL END: Tool {event['name']} output was: {event['data'].get('output')} ---\")\n",
    "\n",
    "# In Jupyter, you can just 'await' the async function\n",
    "await stream_agent_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04008ca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "All connection attempts failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_backends\\auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n\u001b[32m     32\u001b[39m     host,\n\u001b[32m     33\u001b[39m     port,\n\u001b[32m     34\u001b[39m     timeout=timeout,\n\u001b[32m     35\u001b[39m     local_address=local_address,\n\u001b[32m     36\u001b[39m     socket_options=socket_options,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    108\u001b[39m exc_map = {\n\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n\u001b[32m    112\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- TOOL END: Tool \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m output was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# In Jupyter, you can just 'await' the async function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stream_agent_response()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mstream_agent_response\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream_agent_response\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agent_executor.astream_events(\n\u001b[32m     35\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is 3 plus 5, and then multiply the result by 10?\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     36\u001b[39m         version=\u001b[33m\"\u001b[39m\u001b[33mv2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m     ):\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# ... (rest of the streaming logic)\u001b[39;00m\n\u001b[32m     39\u001b[39m         event_type = event[\u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event_type == \u001b[33m\"\u001b[39m\u001b[33mon_chat_model_stream\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1540\u001b[39m, in \u001b[36mRunnable.astream_events\u001b[39m\u001b[34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m   1539\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[32m   1541\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py:1034\u001b[39m, in \u001b[36m_astream_events_implementation_v2\u001b[39m\u001b[34m(runnable, value, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# Await it anyway, to run any cleanup code, and propagate any exceptions\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.suppress(asyncio.CancelledError):\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m task\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py:989\u001b[39m, in \u001b[36m_astream_events_implementation_v2.<locals>.consume_astream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    987\u001b[39m     \u001b[38;5;66;03m# if astream also calls tap_output_aiter this will be a no-op\u001b[39;00m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(runnable.astream(value, config, **kwargs)) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m event_streamer.tap_output_aiter(run_id, stream):\n\u001b[32m    990\u001b[39m             \u001b[38;5;66;03m# All the content will be picked up\u001b[39;00m\n\u001b[32m    991\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py:190\u001b[39m, in \u001b[36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    188\u001b[39m tap = \u001b[38;5;28mself\u001b[39m.is_tapped.setdefault(run_id, sentinel)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m first = \u001b[38;5;28;01mawait\u001b[39;00m py_anext(output, default=sentinel)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\utils\\aiter.py:78\u001b[39m, in \u001b[36mpy_anext.<locals>.anext_impl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manext_impl\u001b[39m() -> Union[T, Any]:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# The C code is way more low-level than this, as it implements\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# all methods of the iterator protocol. In this implementation\u001b[39;00m\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# we're relying on higher-level coroutine concepts, but that's\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# exactly what we want -- crosstest pure-Python high-level\u001b[39;00m\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# implementation and low-level C anext() iterators.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[34m__anext__\u001b[39m(iterator)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1827\u001b[39m, in \u001b[36mAgentExecutor.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1815\u001b[39m config = ensure_config(config)\n\u001b[32m   1816\u001b[39m iterator = AgentExecutorIterator(\n\u001b[32m   1817\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1825\u001b[39m     **kwargs,\n\u001b[32m   1826\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1827\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m   1828\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m step\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent_iterator.py:268\u001b[39m, in \u001b[36mAgentExecutorIterator.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_executor._should_continue(\n\u001b[32m    262\u001b[39m     \u001b[38;5;28mself\u001b[39m.iterations,\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mself\u001b[39m.time_elapsed,\n\u001b[32m    264\u001b[39m ):\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# take the next step: this plans next action, executes it,\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# yielding action and observation as they are generated\u001b[39;00m\n\u001b[32m    267\u001b[39m     next_step_seq: NextStepOutput = []\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_executor._aiter_next_step(\n\u001b[32m    269\u001b[39m         \u001b[38;5;28mself\u001b[39m.name_to_tool_map,\n\u001b[32m    270\u001b[39m         \u001b[38;5;28mself\u001b[39m.color_mapping,\n\u001b[32m    271\u001b[39m         \u001b[38;5;28mself\u001b[39m.inputs,\n\u001b[32m    272\u001b[39m         \u001b[38;5;28mself\u001b[39m.intermediate_steps,\n\u001b[32m    273\u001b[39m         run_manager,\n\u001b[32m    274\u001b[39m     ):\n\u001b[32m    275\u001b[39m         next_step_seq.append(chunk)\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# if we're yielding actions, yield them as they come\u001b[39;00m\n\u001b[32m    277\u001b[39m         \u001b[38;5;66;03m# do not yield AgentFinish, which will be handled below\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:1491\u001b[39m, in \u001b[36mAgentExecutor._aiter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1488\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1490\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._action_agent.aplan(\n\u001b[32m   1492\u001b[39m         intermediate_steps,\n\u001b[32m   1493\u001b[39m         callbacks=run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1494\u001b[39m         **inputs,\n\u001b[32m   1495\u001b[39m     )\n\u001b[32m   1496\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain\\agents\\agent.py:612\u001b[39m, in \u001b[36mRunnableMultiActionAgent.aplan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    604\u001b[39m final_output: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_runnable:\n\u001b[32m    606\u001b[39m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[32m    607\u001b[39m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    610\u001b[39m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[32m    611\u001b[39m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.runnable.astream(\n\u001b[32m    613\u001b[39m         inputs,\n\u001b[32m    614\u001b[39m         config={\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks},\n\u001b[32m    615\u001b[39m     ):\n\u001b[32m    616\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    617\u001b[39m             final_output = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3676\u001b[39m, in \u001b[36mRunnableSequence.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3673\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minput_aiter\u001b[39m() -> AsyncIterator[Input]:\n\u001b[32m   3674\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3676\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.atransform(input_aiter(), config, **kwargs):\n\u001b[32m   3677\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3658\u001b[39m, in \u001b[36mRunnableSequence.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3651\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3652\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34matransform\u001b[39m(\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3656\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   3657\u001b[39m ) -> AsyncIterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3658\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._atransform_stream_with_config(\n\u001b[32m   3659\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3660\u001b[39m         \u001b[38;5;28mself\u001b[39m._atransform,\n\u001b[32m   3661\u001b[39m         patch_config(config, run_name=(config \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name),\n\u001b[32m   3662\u001b[39m         **kwargs,\n\u001b[32m   3663\u001b[39m     ):\n\u001b[32m   3664\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2475\u001b[39m, in \u001b[36mRunnable._atransform_stream_with_config\u001b[39m\u001b[34m(self, inputs, transformer, config, run_type, **kwargs)\u001b[39m\n\u001b[32m   2473\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2474\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2475\u001b[39m         chunk = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(py_anext(iterator), context)\n\u001b[32m   2476\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m   2477\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py:190\u001b[39m, in \u001b[36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    188\u001b[39m tap = \u001b[38;5;28mself\u001b[39m.is_tapped.setdefault(run_id, sentinel)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m first = \u001b[38;5;28;01mawait\u001b[39;00m py_anext(output, default=sentinel)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\utils\\aiter.py:78\u001b[39m, in \u001b[36mpy_anext.<locals>.anext_impl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manext_impl\u001b[39m() -> Union[T, Any]:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# The C code is way more low-level than this, as it implements\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# all methods of the iterator protocol. In this implementation\u001b[39;00m\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# we're relying on higher-level coroutine concepts, but that's\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# exactly what we want -- crosstest pure-Python high-level\u001b[39;00m\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# implementation and low-level C anext() iterators.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[34m__anext__\u001b[39m(iterator)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3625\u001b[39m, in \u001b[36mRunnableSequence._atransform\u001b[39m\u001b[34m(self, inputs, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   3623\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3624\u001b[39m         final_pipeline = step.atransform(final_pipeline, config)\n\u001b[32m-> \u001b[39m\u001b[32m3625\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[32m   3626\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1613\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1610\u001b[39m final: Input\n\u001b[32m   1611\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1613\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[32m   1614\u001b[39m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[32m   1615\u001b[39m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[32m   1616\u001b[39m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[32m   1618\u001b[39m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[32m   1619\u001b[39m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[32m   1621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[32m   1622\u001b[39m         final = ichunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5939\u001b[39m, in \u001b[36mRunnableBindingBase.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5932\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5933\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34matransform\u001b[39m(\n\u001b[32m   5934\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5937\u001b[39m     **kwargs: Any,\n\u001b[32m   5938\u001b[39m ) -> AsyncIterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m5939\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.atransform(\n\u001b[32m   5940\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5941\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5942\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5943\u001b[39m     ):\n\u001b[32m   5944\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1631\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1628\u001b[39m             final = ichunk\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(final, config, **kwargs):\n\u001b[32m   1632\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:615\u001b[39m, in \u001b[36mBaseChatModel.astream\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    613\u001b[39m input_messages = _normalize_messages(messages)\n\u001b[32m    614\u001b[39m run_id = \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m.join((_LC_ID_PREFIX, \u001b[38;5;28mstr\u001b[39m(run_manager.run_id)))\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._astream(\n\u001b[32m    616\u001b[39m     input_messages,\n\u001b[32m    617\u001b[39m     stop=stop,\n\u001b[32m    618\u001b[39m     **kwargs,\n\u001b[32m    619\u001b[39m ):\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.message.id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    621\u001b[39m         chunk.message.id = run_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:983\u001b[39m, in \u001b[36mChatOllama._astream\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_astream\u001b[39m(\n\u001b[32m    977\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    978\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    981\u001b[39m     **kwargs: Any,\n\u001b[32m    982\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiterate_over_stream(messages, stop, **kwargs):\n\u001b[32m    984\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n\u001b[32m    985\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_llm_new_token(\n\u001b[32m    986\u001b[39m                 chunk.text,\n\u001b[32m    987\u001b[39m                 verbose=\u001b[38;5;28mself\u001b[39m.verbose,\n\u001b[32m    988\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:922\u001b[39m, in \u001b[36mChatOllama._aiterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_aiterate_over_stream\u001b[39m(\n\u001b[32m    916\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    917\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    918\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    919\u001b[39m     **kwargs: Any,\n\u001b[32m    920\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m    921\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acreate_chat_stream(messages, stop, **kwargs):\n\u001b[32m    923\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    924\u001b[39m             content = (\n\u001b[32m    925\u001b[39m                 stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    926\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    927\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    928\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:730\u001b[39m, in \u001b[36mChatOllama._acreate_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat(**chat_params):\n\u001b[32m    731\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\ollama\\_client.py:677\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.stream(*args, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    679\u001b[39m       r.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\contextlib.py:214\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_client.py:1583\u001b[39m, in \u001b[36mAsyncClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1561\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m   1562\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1568\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m   1569\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1570\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1571\u001b[39m     method=method,\n\u001b[32m   1572\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1581\u001b[39m     extensions=extensions,\n\u001b[32m   1582\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1583\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(\n\u001b[32m   1584\u001b[39m     request=request,\n\u001b[32m   1585\u001b[39m     auth=auth,\n\u001b[32m   1586\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1587\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1588\u001b[39m )\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1590\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    160\u001b[39m     value = typ()\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\WevN\\WevN_ofiicial_frontend\\WevN\\backend\\server\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers a and b.\"\"\"\n",
    "    print(f\"--- TOOL: Executing multiply({a}, {b}) ---\")\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers a and b.\"\"\"\n",
    "    print(f\"--- TOOL: Executing add({a}, {b}) ---\")\n",
    "    return a + b\n",
    "    \n",
    "# Use the phi3:mini model you pulled \n",
    "# confirmed never use quen3\n",
    "llm = ChatOllama(model=\"qwen3:4b\", temperature=0)\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that is good at math.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "\n",
    "async def stream_agent_response():\n",
    "    async for event in agent_executor.astream_events(\n",
    "        {\"input\": \"What is 3 plus 5, and then multiply the result by 10?\"},\n",
    "        version=\"v2\"\n",
    "    ):\n",
    "        # ... (rest of the streaming logic)\n",
    "        event_type = event[\"event\"]\n",
    "        if event_type == \"on_chat_model_stream\":\n",
    "            chunk = event[\"data\"][\"chunk\"]\n",
    "            if chunk.content:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        elif event_type == \"on_tool_start\":\n",
    "            print(f\"\\n--- TOOL START: {event['name']} with args {event['data'].get('input')} ---\")\n",
    "        elif event_type == \"on_tool_end\":\n",
    "            print(f\"\\n--- TOOL END: Tool {event['name']} output was: {event['data'].get('output')} ---\")\n",
    "\n",
    "# In Jupyter, you can just 'await' the async function\n",
    "await stream_agent_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b982b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
